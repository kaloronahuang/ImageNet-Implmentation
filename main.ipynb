{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalorona-CNN - A ResNet Implementation\n",
    "\n",
    "This notebook aims to modify and test models.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# ResNet model\n",
    "from models.ResNet import resnet50\n",
    "# DALI\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy\n",
    "from nvidia.dali.pipeline import pipeline_def\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali.fn as fn\n",
    "from ImageNetLoader import ImageNetDALIPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "batch_size = 32\n",
    "epoch_limit = 20\n",
    "current_checkpoint = 0\n",
    "\n",
    "dataset_dir = '/mnt/store/ImageNet/dataset/ILSVRC/'\n",
    "img_dir = os.path.join(dataset_dir, 'Data', 'CLS-LOC')\n",
    "annotation_dir = os.path.join(dataset_dir, 'Annotations', 'CLS-LOC', 'val')\n",
    "training_dataset_dir = os.path.join(img_dir, 'train')\n",
    "eval_dataset_dir = os.path.join(img_dir, 'eval')\n",
    "\n",
    "save_dir = './saves/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Utility - Rearrange Eval Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.dom.minidom\n",
    "\n",
    "def rearrange_dataset():\n",
    "    src_path = os.path.join(img_dir, 'val')\n",
    "    dst_path = eval_dataset_dir\n",
    "    for filename in tqdm(os.listdir(annotation_dir)):\n",
    "        xml_path = os.path.join(annotation_dir, filename)\n",
    "        DOMTree = xml.dom.minidom.parse(xml_path)\n",
    "        anno_root = DOMTree.documentElement\n",
    "        image_name = anno_root.getElementsByTagName(\"filename\")[0].firstChild.data\n",
    "        type_name = anno_root.getElementsByTagName(\"object\")[0].getElementsByTagName(\"name\")[0].firstChild.data\n",
    "        # print(f\"image_name : {image_name}, type_name : {type_name}\")\n",
    "        src_image_path = os.path.join(src_path, image_name + \".JPEG\")\n",
    "        dst_image_folder = os.path.join(dst_path, type_name)\n",
    "        if not os.path.exists(dst_image_folder):\n",
    "            os.mkdir(dst_image_folder)\n",
    "        dst_image_path = os.path.join(dst_path, type_name, image_name + \".JPEG\")\n",
    "        os.rename(src_image_path, dst_image_path)\n",
    "\n",
    "rearrange_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create / Load a model\n",
    "\n",
    "### Create a new model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model create.\n"
     ]
    }
   ],
   "source": [
    "model = resnet50().cuda()\n",
    "print('New model create.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_checkpoint = eval(input('Please input the checkpoint ID: '))\n",
    "model_path = os.path.join(save_dir, f'model-ResNet-{current_checkpoint}.pth')\n",
    "print(f'Loading from {model_path}...')\n",
    "model = resnet50()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.cuda()\n",
    "print('Loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Eval\n",
    "\n",
    "### History Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "records = {\n",
    "    'epoch_reports' : [\n",
    "\n",
    "    ],\n",
    "    'eval_reports' : [\n",
    "\n",
    "    ],\n",
    "    'current_iter' : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility - Load records from history file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadHistory():\n",
    "    global records\n",
    "    records = json.load(open('history.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility - Save records to history file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveHistory():\n",
    "    json.dump(records, open('history.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders & Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ImageNetDALIPipeline(batch_size=batch_size,\n",
    "                            num_threads=4,\n",
    "                            device_id=0,\n",
    "                            seed=12,\n",
    "                            data_dir=os.path.join(dataset_dir, \"Data\", \"CLS-LOC\", \"train\"),\n",
    "                            crop=224,\n",
    "                            size=256,\n",
    "                            shard_id=0,\n",
    "                            num_shards=1,\n",
    "                            is_training=True)\n",
    "pipe.build()\n",
    "training_dataloader = DALIClassificationIterator(pipe, reader_name=\"Reader\", last_batch_policy=LastBatchPolicy.PARTIAL)\n",
    "\n",
    "pipe = ImageNetDALIPipeline(batch_size=batch_size,\n",
    "                            num_threads=4,\n",
    "                            device_id=0,\n",
    "                            seed=12,\n",
    "                            data_dir=os.path.join(dataset_dir, \"Data\", \"CLS-LOC\", \"eval\"),\n",
    "                            crop=224,\n",
    "                            size=256,\n",
    "                            shard_id=0,\n",
    "                            num_shards=1,\n",
    "                            is_training=False)\n",
    "pipe.build()\n",
    "eval_dataloader = DALIClassificationIterator(pipe, reader_name=\"Reader\", last_batch_policy=LastBatchPolicy.PARTIAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, enable_log=True):\n",
    "    global current_checkpoint\n",
    "    size = dataloader.size\n",
    "    pgbr = tqdm(total=size)\n",
    "    loss_records = []\n",
    "    print(f'Epoch {current_checkpoint + 1}\\n')\n",
    "\n",
    "    iter_id = records['current_iter']\n",
    "    epoch_report = {'epoch_id': current_checkpoint + 1, 'start_iter': iter_id}\n",
    "    \n",
    "    tic = time.clock()\n",
    "\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        for d in data:\n",
    "            #X, Y = X.cuda(), Y.cuda()\n",
    "            X, Y = d[\"data\"], d[\"label\"].squeeze(-1).long()\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, Y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pgbr.update(batch_size)\n",
    "            iter_id += 1\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "                loss_records.append({'iter': iter_id, 'loss': float(loss)})\n",
    "\n",
    "    toc = time.clock()\n",
    "    \n",
    "    print(f'Epoch {current_checkpoint + 1} Completed')\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'model-ResNet-{current_checkpoint + 1}.pth'))\n",
    "    current_checkpoint += 1\n",
    "\n",
    "    epoch_report['elapse_seconds'] = toc - tic\n",
    "    epoch_report['end_iter'] = iter_id\n",
    "    epoch_report['iter_loss'] = loss_records\n",
    "    if enable_log:\n",
    "        records['current_iter'] = iter_id\n",
    "        records['epoch_reports'].append(epoch_report)\n",
    "        SaveHistory()\n",
    "    print(f'Model & Record file saved. Elapsed Time: {toc - tic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, enable_log=True):\n",
    "    global current_checkpoint\n",
    "    size = dataloader.size\n",
    "    num_batches = 0\n",
    "    test_loss, correct = 0, 0\n",
    "    pgbr = tqdm(size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, data in enumerate(dataloader):\n",
    "            for d in data:\n",
    "                X, y = d[\"data\"], d[\"label\"].squeeze(-1).long()\n",
    "                pred = model(X)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                num_batches += 1\n",
    "                pgbr.update(batch_size)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if enable_log:\n",
    "        eval_report = {'eval_checkpoint': current_checkpoint, 'avg_loss': float(test_loss), 'accuracy': float(100 * correct), 'time': time.asctime()}\n",
    "        records['eval_reports'].append(eval_report)\n",
    "        SaveHistory()\n",
    "        print('Eval report saved, history file saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operate\n",
    "\n",
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                          | 0/1281167 [00:00<?, ?it/s]/home/kalorona/anaconda3/envs/DL/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 64/1281167 [00:01<5:25:50, 65.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.222864 [    0/1281167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–Ž                                                                                                           | 3136/1281167 [00:26<2:59:33, 118.62it/s]"
     ]
    }
   ],
   "source": [
    "while current_checkpoint < epoch_limit:\n",
    "    train_loop(training_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(eval_dataloader, model, loss_fn)\n",
    "    current_checkpoint += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(eval_dataloader, model, loss_fn, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawLossGraph():\n",
    "    epoch_reports = records['epoch_reports']\n",
    "    xpts, ypts = [], []\n",
    "    for report in epoch_reports:\n",
    "        for loss_rec in report['iter_loss']:\n",
    "            for iter_loss in loss_rec:\n",
    "                xpts.append(iter_loss['iter']), ypts.append(iter_loss['loss'])\n",
    "    plt.plot(np.array(xpts), np.array(ypts))\n",
    "    plt.show()\n",
    "\n",
    "DrawLossGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawAccuracyGraph():\n",
    "    eval_reports = records['eval_reports']\n",
    "    xpts, ypts = [], []\n",
    "    for report in eval_reports:\n",
    "        xpts.append(report['eval_checkpoint']), ypts.append(report['accuracy'])\n",
    "    plt.plot(np.array(xpts), np.array(ypts))\n",
    "    plt.show()\n",
    "\n",
    "DrawAccuracyGraph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "036d3f45127f0ed64e82e8b1862376ff644019806cd3466a7acd49ebdbc113ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
